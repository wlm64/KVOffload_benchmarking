vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 --no-enable-prefix-caching --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}' --max-num-batched-tokens 131072 --max_model_len 131072 --gpu_memory_utilization .90 --tensor-parallel-size 8
